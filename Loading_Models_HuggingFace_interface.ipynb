{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4a58e1c-8dd0-449f-b176-935a27d70b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: transformers in /opt/anaconda3/lib/python3.12/site-packages (4.35.2)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (1.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2025.4.26)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a26bde4-79d4-4929-a9f8-5d0a45306b02",
   "metadata": {},
   "source": [
    "## Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd59acea-d538-4d48-8792-03a848cbe380",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "\n",
    "\n",
    "def warn(*args,**kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eda1da9-d8ce-4fdb-a89b-1186e5ba78f9",
   "metadata": {},
   "source": [
    "## Text Classification with DistilBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1aa8ae-b6eb-4d7c-aecd-369c4492e683",
   "metadata": {},
   "source": [
    "#### Load the model and the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7f1238d-c0a9-413d-8ff7-dadb0c2d3900",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dab732eb00b426a9fc1650f51d3037f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english/resolve/main/model.safetensors: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4980f4a2786241cab3f3f3a8187c5fe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   8%|7         | 21.0M/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english/resolve/main/model.safetensors: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7364811f7fb5473d8c0fadea4903b767",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  31%|###1      | 83.9M/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english/resolve/main/model.safetensors: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fd709f322a04f8db11052fb4895b0ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  31%|###1      | 83.9M/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the tokenizer and the model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c75f8d-9b31-498d-880c-12febef812d4",
   "metadata": {},
   "source": [
    "## Process the input text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "729446db-4377-40e9-a555-bd50c6ec5577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101, 23156,   999,  2017,  1005,  2310,  2180,  1037,  2489,  7281,\n",
      "          2000,  1996, 17094,  1012,  7514,  2663,  2000,  4366,  1012,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# sample text\n",
    "text = \"Congratulations! You've won a free ticket to the Bahamas. Reply WIN to claim.\"\n",
    "\n",
    "# tokenize the input text\n",
    "tokens = tokenizer(text,return_tensors='pt')\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "193ecb23-1324-41ad-9677-c4ad14b81a55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101, 23156,   999,  2017,  1005,  2310,  2180,  1037,  2489,  7281,\n",
       "          2000,  1996, 17094,  1012,  7514,  2663,  2000,  4366,  1012,   102]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2a9d05-4f23-410b-87ac-21244176b3e1",
   "metadata": {},
   "source": [
    "attention_mask is essential for correctly processing padded sequences, ensuring efficient computation, and maintaining model performance. \n",
    "Even when no tokens are explicitly masked, it helps the model differentiate between actual content and padding, which is critical for accurate and efficient processing of input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0240bfd3-b6ac-486f-9600-3b234e70d9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(input_ids = tokens[\"input_ids\"], attention_mask = tokens[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "28f4cbd3-785d-4566-9bef-7d0b540c28fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-3.9954,  4.3336]]), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b3ca18fe-8cd9-4203-9c4b-b830b1e7c623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = outputs.logits\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b33aa57-0df5-498c-b921-9772f5acd50c",
   "metadata": {},
   "source": [
    "## Post Process the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "46419ad5-3333-49b5-86a8-3888894566a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label: POSITIVE\n"
     ]
    }
   ],
   "source": [
    "# Convert logits to probabilities\n",
    "probs = torch.softmax(logits,dim = -1)\n",
    "\n",
    "# get the predicted class\n",
    "predicted_class = torch.argmax(probs, dim = -1)\n",
    "\n",
    "# Map the predicted class to the label\n",
    "labels = [\"NEGATIVE\",\"POSITIVE\"]\n",
    "predicted_label = labels[predicted_class]\n",
    "\n",
    "print(f\"Predicted label: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d435d4-5a09-4eb1-a113-ae1b51d25513",
   "metadata": {},
   "source": [
    "## Text Generation with GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "02c5d011-178e-4b03-b29c-bc51f4bf9d9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f98942765b34447baad0f29a0c538a37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc6d785d5621438ea6df5ecab83f47f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2f069fc0e844f63aab934aa2c9c4efa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34287cffafee4040b75e662457073144",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44e0644ed3594aacb51dfec78e37859e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the tokenizer and the model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7bc5b7c9-52e7-4cc2-a787-33ea557be783",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a061e5bc73e04e9982fc7f9a41030029",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d05bcfe30ab34af6a5f432621b7deede",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ca0d8044-aa52-4217-842b-d1542a62e084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[7454, 2402,  257,  640]]), 'attention_mask': tensor([[1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Process the input text\n",
    "prompt = \"Once upon a time\"\n",
    "\n",
    "prompt_encoded = tokenizer(prompt,return_tensors = 'pt')\n",
    "\n",
    "prompt_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e0f457-e725-4c19-b250-0f0e07084c3f",
   "metadata": {},
   "source": [
    "## Perform Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53aa8ebb-0630-419c-bd0b-abde35b15608",
   "metadata": {},
   "source": [
    "* `inputs:` input token IDs form tokenizer\n",
    "* `attention_mask:` mask indicating which tokens to attend to\n",
    "* `pad_token:` Padding token ID set to the end of sequence token ID\n",
    "* `max_length:` Maximum Length of the generated sequences\n",
    "* `num_return_sequence:` Number of sequences to generate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4181bd14-542a-468c-9b5b-d5567f3047fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate text\n",
    "generate_text = model.generate(\n",
    "    prompt_encoded.input_ids,\n",
    "    attention_mask = prompt_encoded.attention_mask,\n",
    "    pad_token_id = tokenizer.eos_token_id,\n",
    "    max_length = 50,\n",
    "    num_return_sequences = 1\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "38c7882b-952f-4208-afa7-febf2ec7eb05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7454, 2402,  257,  640,   11,  262,  995,  373,  257, 1295,  286, 1049,\n",
       "         8737,  290, 1049, 3514,   13,  383,  995,  373,  257, 1295,  286, 1049,\n",
       "         3514,   11,  290,  262,  995,  373,  257, 1295,  286, 1049, 3514,   13,\n",
       "          383,  995,  373,  257, 1295,  286, 1049, 3514,   11,  290,  262,  995,\n",
       "          373,  257]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7b6a2bc1-571d-4c44-9acf-a3bdb4562241",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**prompt_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f5a70a6e-0c1c-4173-8456-d63cb779d47b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -34.5646,  -34.4082,  -38.3080,  ...,  -41.6997,  -39.7802,\n",
       "           -35.0522],\n",
       "         [ -84.7255,  -82.9326,  -87.0165,  ...,  -91.6667,  -86.2354,\n",
       "           -84.7094],\n",
       "         [-109.0799, -105.7258, -109.9115,  ..., -114.2847, -107.6933,\n",
       "          -105.3613],\n",
       "         [ -57.8935,  -58.5540,  -64.7374,  ...,  -64.9437,  -62.9294,\n",
       "           -60.0624]]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "acac150c-9010-46be-9c08-e806ddb18d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a\n"
     ]
    }
   ],
   "source": [
    "# decode the generated text\n",
    "text_output = tokenizer.decode(generate_text[0], skip_special_tokens = True)\n",
    "print(text_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b022332-1f24-4d88-94a9-a3de2a818541",
   "metadata": {},
   "source": [
    "## Hugging Face Pipeline() Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae981114-590d-48ff-b50d-1311b832900d",
   "metadata": {},
   "source": [
    "The `pipeline()` function from the huggingface transformers library  is a high level API designed to simplify the usage of pretrained models for various natural language processing (NLP) task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5144e9c2-7802-48c3-9a9d-6567040f4dbc",
   "metadata": {},
   "source": [
    "transformers.pipeline(\n",
    "    \n",
    "    task: str,\n",
    "    model: Optional = None,\n",
    "    config: Optional = None,\n",
    "    tokenizer: Optional = None,\n",
    "    feature_extractor: Optional = None,\n",
    "    framework: Optional = None,\n",
    "    revision: str = 'main',\n",
    "    use_fast: bool = True,\n",
    "    model_kwargs: Dict[str, Any] = None,\n",
    "    **kwargs\n",
    ")\n",
    "\n",
    "**Parameters**\n",
    "\n",
    "* task: str\n",
    "  * The task to perform, such as \"text classification\", \"text-generation\", \"question-answering\"\n",
    "  * example: \"text-classification\"\n",
    "* model: Optional\n",
    "    *  The model to use. This can be string (model identifier from huggingface model hub), a path to a directory\n",
    " \n",
    "\n",
    "* **Task types**\n",
    "    1. Text Classification:\n",
    "    2. Text Generation:\n",
    "    3. Q-A answering\n",
    "    4. summarization\n",
    "    5. Translation\n",
    "    6. fill mask\n",
    "    7. zero shot classification\n",
    "    8. feature extraction\n",
    "    9. named entity recognition  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d576d4-c207-4b90-99f8-4e539fd37cef",
   "metadata": {},
   "source": [
    "## Text Classification using pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7919196a-b00b-4d18-91ad-9fed1b781b46",
   "metadata": {},
   "source": [
    "Initialize the pipeline for text classification task. load a pretrained text classification model and use it to classify a sample text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "df2910bb-96e4-48c3-bfee-9a9e1a74ce76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9998331069946289}]\n"
     ]
    }
   ],
   "source": [
    "## load the text classification model\n",
    "classifier = pipeline(task = \"text-classification\",model = \"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "## Classify a sample task\n",
    "result = classifier(\"Congratulations! You've won the lottery\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1280230-b5f5-4713-9a04-67d708c2de14",
   "metadata": {},
   "source": [
    "The output will return a dictionary, where each dictionary contains\n",
    "* label: the predicted label\n",
    "* scores: the confidence score of predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b707d3-2903-4836-9965-a19101f363aa",
   "metadata": {},
   "source": [
    "## Language detection using Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "829b287e-cc9f-4b73-9325-35915c088fc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe24c231ea78429fafe809c949c2f0f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef249c56fafb421e9cb9546c5ca454d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://huggingface.co/papluca/xlm-roberta-base-language-detection/resolve/main/model.safetensors: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05996f34a1784340b98b397c5dfd2111",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   2%|1         | 21.0M/1.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a78ae800a7454e6f8e85b3ad1c76bd86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27d5776384e1419cb4bc805006769066",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e54ca5b382b40bc95059aa9de32e1d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15b698e03c3a4daab17b0359a478c4b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'fr', 'score': 0.9924910664558411}]\n"
     ]
    }
   ],
   "source": [
    "# load the classifier\n",
    "classifier = pipeline(task = 'text-classification', model = 'papluca/xlm-roberta-base-language-detection')\n",
    "result = classifier('Bonjour, comment ca va?')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e55e53e-8bb7-4c31-8815-f542f3d67ead",
   "metadata": {},
   "source": [
    "## Text Generation Using Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0d0bae37-97ad-4433-94d5-95d2ba1b232e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "once upon a time.\n",
      "\n",
      "I will not be silent. I will not say something unteniably. This is what was seen.\n",
      "\n",
      "But you cannot hide from it.\n",
      "\n",
      "You said, \"Do not let me be surprised;\n"
     ]
    }
   ],
   "source": [
    "## text generation using pipeline\n",
    "generator = pipeline(task = 'text-generation', model = 'gpt2')\n",
    "\n",
    "prompt = 'once upon a time'\n",
    "\n",
    "result = generator(prompt,max_length = 50,num_return_sequences = 1)\n",
    "\n",
    "# print the generated text\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db960ca-a496-4b65-b12f-adbec1861714",
   "metadata": {},
   "source": [
    "if we use:\n",
    "print(result) --> we might get this\n",
    "[{'generated_text': 'Once upon a time there was a little rabbit who loved...'}]\n",
    "\n",
    "result -> a list\n",
    "result[0] -> the first dictionary in that list\n",
    "result[0]['generated_text'] -> the actual generated string\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41dcec0-72a6-485b-9ef0-89e2fea87704",
   "metadata": {},
   "source": [
    "## Text generation using t5 with pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e7ad0d58-2fb8-4d72-9d9c-b3026c2568dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13fa3af2094646f59b54a9754939d57b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5102c5849a64755bfa019996aaadc0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95c54c129ffc40bbac7ee23ee62f8def",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e694aabe8ed41e8966b98f68edacc9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd756eb68bcd431681f13412f77ef606",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d45aa06db8a4efcbd60973fa7325960",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": Comment êtes-vous?\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline(task = 'text2text-generation', model = 't5-small')\n",
    "prompt = 'translate: English to French : How are you?'\n",
    "\n",
    "result = generator(prompt,max_length =50, num_return_sequences = 1)\n",
    "\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbc5cc6-5530-4ca9-b73f-19b0fb4ee854",
   "metadata": {},
   "source": [
    "## Fill Mask task by pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fd684c8d-4da7-46eb-a6bc-6abbcc03e474",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'bert.pooler.dense.bias', 'cls.seq_relationship.weight', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the capital of france is. (score: 0.7154)\n",
      "the capital of france is ; (score: 0.2527)\n",
      "the capital of france is | (score: 0.0281)\n",
      "the capital of france is! (score: 0.0021)\n",
      "the capital of france is? (score: 0.0013)\n"
     ]
    }
   ],
   "source": [
    "mask_filler = pipeline(task = 'fill-mask', model = 'bert-base-uncased')\n",
    "\n",
    "prompt = \"The capital of France is [MASK]\"\n",
    "\n",
    "result = mask_filler(prompt)\n",
    "\n",
    "for r in result:\n",
    "    print(f\"{r['sequence']} (score: {r['score']:.4f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
